{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important:** The model estimation code is intended to work with an experimental parallelised Vensim engine. With appropriate modifications to the main function calls (but not the analytical procedure), the same analysis can be run on regular commercially available Vensim DSS, though it will take *much* longer. Please contact [Tom Fiddaman](mailto:tom@ventanasystems.com) for information on the experimental Vensim engine.\n",
    "\n",
    "For more information on the model estimation procedure, see S1 of the Supplementary Materials of the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import regex\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "import matplotlib.ticker as mticker\n",
    "import matplotlib.dates as mdates\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from shutil import copy\n",
    "from scipy.stats import pearsonr\n",
    "from distutils.dir_util import copy_tree\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "##### CLASS & FUNCTION DEFINITIONS FOR WORKING WITH VENSIM/VENGINE #####\n",
    "\n",
    "class Script(object):\n",
    "    \"\"\"Master object for holding and modifying .cmd script settings, \n",
    "    creating .cmd files, and running them through Vensim/Vengine\"\"\"\n",
    "    def __init__(self, controlfile):\n",
    "        print(\"Initialising\", self)\n",
    "        for k, v in controlfile['simsettings'].items():\n",
    "            self.__setattr__(k, v if isinstance(v, str) else v.copy())\n",
    "        self.setvals = []\n",
    "        self.runcmd = \"MENU>RUN_OPTIMIZE|o\\n\"\n",
    "        self.savecmd = f\"MENU>VDF2TAB|!|!|{self.savelist}|\\n\"\n",
    "        self.basename = controlfile['baserunname']\n",
    "        self.cmdtext = []\n",
    "        \n",
    "    def copy_model_files(self, dirname):\n",
    "        \"\"\"Create subdirectory and copy relevant model files to it,\n",
    "        then change working directory to subdirectory\"\"\"\n",
    "        os.makedirs(dirname, exist_ok=True)\n",
    "        os.chdir(f\"./{dirname}\")\n",
    "\n",
    "        # Copy needed files from the working directory into the sub-directory\n",
    "        for s in ['model', 'payoff', 'optparm', 'sensitivity', 'savelist', 'senssavelist']:\n",
    "            if getattr(self, s):\n",
    "                copy(f\"../{getattr(self, s)}\", \"./\")\n",
    "        for slist in ['data', 'changes']:\n",
    "            for file in getattr(self, slist):\n",
    "                copy(f\"../{file}\", \"./\")\n",
    "            \n",
    "    def add_suffixes(self, settingsfxs):\n",
    "        \"\"\"Cleanly modifies .cmd script settings with specified suffixes\"\"\"\n",
    "        for s, sfx in settingsfxs.items():\n",
    "            if hasattr(self, s):\n",
    "                self.__setattr__(s, getattr(self, s)[:-4] + sfx + getattr(self, s)[-4:])\n",
    "   \n",
    "    def update_changes(self, chglist, setvals=[]):\n",
    "        \"\"\"Reformats chglist as needed to extend changes settings; \n",
    "        see compile_script for details\"\"\"\n",
    "        # Combines and flattens list of paired change names & suffixes\n",
    "        flatlist = [i for s in \n",
    "                    [[f\"{self.basename}_{n}_{sfx}.out\" for n in name] \n",
    "                     if isinstance(name, list) else [f\"{self.basename}_{name}_{sfx}.out\"] \n",
    "                     for name, sfx in chglist] for i in s]\n",
    "        self.changes.extend(flatlist)\n",
    "        self.setvals = setvals\n",
    "          \n",
    "    def write_script(self, scriptname):\n",
    "        \"\"\"Compiles and writes actual .cmd script file\"\"\"\n",
    "        self.cmdtext.extend([\"SPECIAL>NOINTERACTION\\n\", \n",
    "                             f\"SPECIAL>LOADMODEL|{self.model}\\n\"])\n",
    "        \n",
    "        for s in ['payoff', 'sensitivity', 'optparm', 'savelist', 'senssavelist']:\n",
    "            if hasattr(self, s):\n",
    "                self.cmdtext.append(f\"SIMULATE>{s}|{getattr(self, s)}\\n\")\n",
    "        \n",
    "        if hasattr(self, 'data'):\n",
    "            datatext = ','.join(self.data)\n",
    "            self.cmdtext.append(f\"SIMULATE>DATA|\\\"{','.join(self.data)}\\\"\\n\")\n",
    "\n",
    "        if self.changes:\n",
    "            self.cmdtext.append(f\"SIMULATE>READCIN|{self.changes[0]}\\n\")\n",
    "            for file in self.changes[1:]:\n",
    "                self.cmdtext.append(f\"SIMULATE>ADDCIN|{file}\\n\")\n",
    "        \n",
    "        self.cmdtext.extend([\"\\n\", f\"SIMULATE>RUNNAME|{scriptname}\\n\", \n",
    "                             self.runcmd, self.savecmd, \n",
    "                             \"SPECIAL>CLEARRUNS\\n\", \"MENU>EXIT\\n\"])\n",
    "        \n",
    "        with open(f\"{scriptname}.cmd\", 'w') as scriptfile:\n",
    "            scriptfile.writelines(self.cmdtext)\n",
    "    \n",
    "    def run_script(self, scriptname, controlfile, subdir, logfile):\n",
    "        \"\"\"Runs .cmd script file using function robust to \n",
    "        Vengine errors, and returns payoff value if applicable\"\"\"\n",
    "        return run_vengine_script(scriptname, controlfile['vensimpath'], \n",
    "                                  controlfile['timelimit'], '.log', check_opt, logfile)\n",
    "\n",
    "    \n",
    "class CtyScript(Script):\n",
    "    \"\"\"Script subclass for country optimization runs\"\"\"\n",
    "    def __init__(self, controlfile):\n",
    "        super().__init__(controlfile)\n",
    "        self.genparams = controlfile['analysissettings']['genparams'].copy()\n",
    "        \n",
    "    def prep_subdir(self, scriptname, controlfile, subdir):\n",
    "        \"\"\"Creates subdirectory for country-specific files and output\"\"\"\n",
    "        self.copy_model_files(subdir)\n",
    "        copy(f\"../{scriptname}.cmd\", \"./\")\n",
    "        self.genparams.append(f\"[{subdir}]\")\n",
    "        for file in self.changes:\n",
    "            if 'main' in file:\n",
    "                clean_outfile(file, self.genparams)\n",
    "            \n",
    "    def run_script(self, scriptname, controlfile, subdir, logfile):\n",
    "        self.prep_subdir(scriptname, controlfile, subdir)\n",
    "        res = run_vengine_script(scriptname, controlfile['vensimpath'], \n",
    "                                 controlfile['timelimit'], '.log', check_opt, logfile)\n",
    "        copy(f\"./{scriptname}.out\", \"..\") # Copy the .out file to parent directory\n",
    "        os.chdir(\"..\")\n",
    "        return res\n",
    "\n",
    "\n",
    "def compile_script(controlfile, scriptclass, name, namesfx, settingsfxs, \n",
    "                   logfile, chglist=[], setvals=[], subdir=None):\n",
    "    \"\"\"Master function for assembling & running .cmd script\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    controlfile : JSON object\n",
    "        Master control file specifying sim settings, runname, etc.\n",
    "    scriptclass : Script object\n",
    "        Type of script object to instantiate, depending on run type\n",
    "    name : str\n",
    "    namesfx : str\n",
    "        Along with `name`, specifies name added to baserunname for run\n",
    "    settingsfxs : dict of str\n",
    "        Dict of suffixes to append to filenames in simsettings; use to \n",
    "        distinguish versions of e.g. .mdl, .voc, .vpd etc. files\n",
    "    logfile : str of filename/path\n",
    "    chglist : list of tuples of (str or list, str)\n",
    "        Specifies changes files to be used in script; specify as tuples \n",
    "        corresponding to `name`, `namesfx` of previous run .out to use; \n",
    "        tuples can also take a list of `names` as first element, taking \n",
    "        each with the same second element; if used with ScenScript run, \n",
    "        `chglist` can also take one non-tuple str as its last element, \n",
    "        which will be added directly (e.g. .cin files for scenarios)\n",
    "    setvals : list of tuples of (str, int or float, <str>)\n",
    "        Specifies variables and values to change for a given run using \n",
    "        Vensim's SETVAL script command; by default all SETVAL commands \n",
    "        will be implemented together for main run, but if `scriptclass` \n",
    "        is MultiScript, each SETVAL command will be implemented and run \n",
    "        separately in sequence; if used with MultiScript, each tuple in \n",
    "        `setvals` will require a third str element specifying the suffix \n",
    "        with which to save the run\n",
    "    subdir : str, optional\n",
    "        Name of subdirectory to create/use for run, if applicable\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Payoff value of the script run, if applicable, else 0\n",
    "    \"\"\"\n",
    "    mainscript = scriptclass(controlfile)\n",
    "    mainscript.add_suffixes(settingsfxs)\n",
    "    mainscript.update_changes(chglist, setvals)\n",
    "    scriptname = f\"{mainscript.basename}_{name}_{namesfx}\"    \n",
    "    mainscript.write_script(scriptname)\n",
    "    return mainscript.run_script(scriptname, controlfile, subdir, logfile)\n",
    "\n",
    "\n",
    "def check_opt(scriptname, logfile):\n",
    "    \"\"\"Check function for use with run_vengine_script for optimizations\"\"\"\n",
    "    if check_zeroes(scriptname):\n",
    "        write_log(f\"Help! {scriptname} is being repressed!\", logfile)\n",
    "    return not check_zeroes(scriptname)\n",
    "\n",
    "\n",
    "def run_vengine_script(scriptname, vensimpath, timelimit, checkfile, check_func, logfile):\n",
    "    cmd_file_path = f\"./{scriptname}.cmd\"\n",
    "    print(\"Command file path: \", cmd_file_path)\n",
    "        \n",
    "    if os.path.exists(cmd_file_path) and os.path.exists(vensimpath):\n",
    "        write_log(f\"Running {scriptname}\", logfile)\n",
    "        subprocess.run([vensimpath, cmd_file_path])\n",
    "    else:\n",
    "        write_log(\"Error: Either the command file or the Vensim executable path does not exist.\", logfile)\n",
    "\n",
    "\n",
    "def clean_outfile(outfilename, linekey):\n",
    "    \"\"\"Clean an outfile to include only lines containing a string in \n",
    "    `linekey`, which should be a list of strings to keep\"\"\"\n",
    "    with open(outfilename,'r') as f:\n",
    "        filedata = f.readlines()\n",
    "\n",
    "    newdata = [line for line in filedata if any(k in line for k in linekey)]\n",
    "    \n",
    "    with open(outfilename, 'w') as f:\n",
    "        f.writelines(newdata)\n",
    "\n",
    "\n",
    "def check_zeroes(scriptname):\n",
    "    \"\"\"Check if an .out file has any parameters set to zero (indicates \n",
    "    Vengine error), return True if any parameters zeroed OR if # runs = \n",
    "    # restarts, and False otherwise\"\"\"\n",
    "    filename = f\"{scriptname}.out\"\n",
    "    with open(filename,'r') as f0:\n",
    "        filedata = f0.readlines()\n",
    "    \n",
    "    checklist = []\n",
    "    for line in filedata:\n",
    "        if line[0] != ':':\n",
    "            if ' = 0 ' in line:\n",
    "                checklist.append(True)\n",
    "            else:\n",
    "                checklist.append(False)\n",
    "        elif ':RESTART_MAX' in line:\n",
    "            restarts = regex.findall(r'\\d+', line)[0]\n",
    "    \n",
    "    # Ensure number of simulations != number of restarts\n",
    "    if f\"After {restarts} simulations\" in filedata[0]:\n",
    "        checklist.append(True)\n",
    "        \n",
    "    # Ensure payoff is not erroneous\n",
    "    if abs(read_payoff(filename)) == 1.29807e+33:\n",
    "        checklist.append(True)\n",
    "    \n",
    "    return any(checklist)\n",
    "\n",
    "\n",
    "def write_log(string, logfile):\n",
    "    \"\"\"Writes printed script output to a logfile\"\"\"\n",
    "    with open(logfile,'a') as f:\n",
    "        f.write(string + \"\\n\")\n",
    "    print(string)\n",
    "\n",
    "    \n",
    "def modify_mdl(country, finaltime, modelname, newmodelname):\n",
    "    \"\"\"Opens .mdl as text, identifies Rgn subscript, and replaces \n",
    "    with appropriate country name\"\"\"\n",
    "    with open(modelname,'r') as f:\n",
    "        filedata = f.read()\n",
    "        \n",
    "    rgnregex = regex.compile(r\"Rgn(\\s)*?:(\\n)?[\\s\\S]*?(\\n\\t~)\")\n",
    "    timeregex = regex.compile(r\"FINAL TIME\\s*=\\s*\\d*\\n\")\n",
    "    tempdata = rgnregex.sub(f\"Rgn:\\n\\t{country}\\n\\t~\", filedata)\n",
    "    newdata = timeregex.sub(f\"FINAL TIME = {finaltime}\\n\", tempdata)\n",
    "\n",
    "    with open(newmodelname,'w') as f:\n",
    "        f.write(newdata)\n",
    "\n",
    "\n",
    "def split_voc(vocname, mcsettings, fixparams):\n",
    "    \"\"\"Splits .VOC file into multiple versions, for main, country, \n",
    "    initial, full model, general MCMC, and country MCMC calibration\"\"\"\n",
    "    with open(vocname,'r') as f0:\n",
    "        filedata = f0.readlines()\n",
    "    \n",
    "    voccty = [line for line in filedata if line[0] == ':' or '[Rgn' in line]\n",
    "\n",
    "    # Make necessary substitutions for MCMC settings\n",
    "    vocctymc = ''.join(voccty)\n",
    "    for k,v in mcsettings.items():\n",
    "        vocctymc = regex.sub(f\":{regex.escape(k)}=.*\", f\":{k}={v}\", vocctymc)\n",
    "    \n",
    "    textlist = [voccty, vocctymc]\n",
    "    sfxs = ['c', 'cmc']\n",
    "    \n",
    "    for k, subs in fixparams.items():\n",
    "        lines = filedata.copy()\n",
    "        for l, line in enumerate(lines):\n",
    "            if ':MULTIPLE_START' in line:\n",
    "                lines[l] = ':MULTIPLE_START=OFF\\n'\n",
    "        text = ''.join(lines)\n",
    "        for sub, val in subs.items():\n",
    "            text = regex.sub(f\".*{regex.escape(sub)}.*\", f\"{val}<={sub}<={val}\", text)\n",
    "        textlist.append(text)\n",
    "        sfxs.append(k)\n",
    "        \n",
    "    # Write various voc versions to separate .voc files\n",
    "    for fname, suffix in zip(textlist, sfxs):\n",
    "        with open(f\"{vocname[:-4]}_{suffix}.voc\", 'w') as f:\n",
    "            f.writelines(fname)\n",
    "\n",
    "\n",
    "def create_mdls(controlfile, countrylist, finaltime, logfile):\n",
    "    \"\"\"Creates copies of the base .mdl file for each country in list \n",
    "    (and one main copy) and splits .VOC files\"\"\"\n",
    "    model = controlfile['simsettings']['model']\n",
    "    for c in countrylist:\n",
    "        newmodel = model[:-4] + f'_{c}.mdl'\n",
    "        modify_mdl(c, finaltime, model, newmodel)\n",
    "\n",
    "    mainmodel = model[:-4] + '_main.mdl'\n",
    "    c_list = [f'{c}\\\\\\n\\t\\t' if i % 10 == 9 else c for i,c in enumerate(countrylist)]\n",
    "    countrylist_str = str(c_list)[1:-1].replace(\"'\",\"\")\n",
    "    modify_mdl(countrylist_str, finaltime, model, mainmodel)\n",
    "    split_voc(controlfile['simsettings']['optparm'], \n",
    "              controlfile['mcsettings'], controlfile['fixparams'])\n",
    "    write_log(\"Files are ready! moving to calibration\", logfile)\n",
    "\n",
    "\n",
    "def read_payoff(outfile, line=1):\n",
    "    \"\"\"Identifies payoff value from .OUT or .REP file - \n",
    "    use line 1 (default) for .OUT, or use line 0 for .REP\"\"\"\n",
    "    with open(outfile, 'r') as f:\n",
    "        payoffline = f.readlines()[line]\n",
    "    payoffvalue = [float(s) for s in \n",
    "                   regex.findall(r'-?(?:0|[1-9]\\d*)(?:\\.\\d*)?(?:[eE][+\\-]?\\d+)?', payoffline)][0]\n",
    "    return payoffvalue\n",
    "\n",
    "\n",
    "def read_outvals(outfile):\n",
    "    \"\"\"Converts .out file into list of tuples of var names & values\"\"\"\n",
    "    with open(outfile, 'r') as f:\n",
    "        output = [line for line in f.readlines() if (line[0] != ':')]\n",
    "\n",
    "    names = [line.split('<=')[1].split('=')[0].strip() for line in output]\n",
    "    values = [float(line.split('<=')[1].split('=')[1]) for line in output]\n",
    "    \n",
    "    return list(zip(names, values))\n",
    "\n",
    "\n",
    "##### FUNCTION DEFINITIONS FOR ANALYSIS & DATA PROCESSING #####\n",
    "\n",
    "def get_first_idx(s, threshold):\n",
    "    \"\"\"Return index of first value in series `s` above `threshold`\"\"\"\n",
    "    return (s > threshold).idxmax(skipna=True)\n",
    "\n",
    "\n",
    "def calc_mean(resdf, var, limit=180):\n",
    "    \"\"\"Return mean of `var` over historical period given by `limit`\"\"\"\n",
    "    limit = min(len(resdf.loc[var]), limit)\n",
    "    val = resdf.loc[var][-limit:].mean()\n",
    "    return val\n",
    "\n",
    "\n",
    "def calc_gof(resdf, simvar, datavar):\n",
    "    \"\"\"Calculate goodness-of-fit measures for given sim & data vars\"\"\"\n",
    "    # IMPORTANT: cross-screen for missing sim or data values\n",
    "    sim = resdf.loc[simvar].where(resdf.loc[datavar].notna())\n",
    "    dat = resdf.loc[datavar].where(resdf.loc[simvar].notna())\n",
    "    \n",
    "    # Calculate various GOF stats & return each one\n",
    "    error = abs(sim - dat)\n",
    "    try:\n",
    "        maen = error.mean()/dat.mean()\n",
    "    except ZeroDivisionError:\n",
    "        maen = np.nan\n",
    "    mape = (error/dat).mean()\n",
    "    simstd = np.sqrt((sim ** 2).mean() - sim.mean() ** 2)\n",
    "    datastd = np.sqrt((dat ** 2).mean() - dat.mean() ** 2)\n",
    "    r2 = (sim.corr(dat)) ** 2\n",
    "    mse = (error ** 2).mean()\n",
    "    um = ((sim.mean() - dat.mean()) ** 2/ mse)\n",
    "    us = ((simstd - datastd) ** 2/ mse)\n",
    "    uc = (2 * (1 - sim.corr(dat)) * simstd * datastd / mse)\n",
    "    return maen, mape, r2, mse, um, us, uc\n",
    "\n",
    "\n",
    "def trunc_log(df):\n",
    "    \"\"\"Return log10 of a dataframe, ignoring negative base values\"\"\"\n",
    "    df[df <= 0] = np.NaN\n",
    "    return np.log10(df)\n",
    "\n",
    "\n",
    "def clean_subscripts(varname):\n",
    "    \"\"\"Removes 3-letter ISO codes from subscripts in `varname`\"\"\"\n",
    "    name = regex.sub(r\"\\[[A-Z]{3}]\", \"\", varname)\n",
    "    name = regex.sub(r\"\\[[A-Z]{3},\\s?\", \"[\", name)\n",
    "    return name\n",
    "\n",
    "\n",
    "def process_results(scriptname, gof_vars, c, iqr_list=[], means_list=[], hist_windows=[180], \n",
    "                    perc_list=[0.05,0.95], delta=None, duration=None):\n",
    "    \"\"\"Read single-country calibration results and calculate additional \n",
    "    outputs, incl. percentiles and IQRs, returning a compiled pd.Series \n",
    "    of processed country results `c_res` and a Dataframe with country \n",
    "    time series outputs (deaths & infs) `datdf`\"\"\"\n",
    "    # Read country parameter values from .out file\n",
    "    outlist = read_outvals(f'{scriptname}.out')\n",
    "    varnames = [clean_subscripts(var[0]) for var in outlist]\n",
    "    vals = [var[1] for var in outlist]\n",
    "    c_res = pd.Series(vals, index=varnames)\n",
    "    \n",
    "    # Read full country calibration results, extract death & inf data for output\n",
    "    resdf = pd.read_csv(f'{scriptname}.tab', sep='\\t', index_col=0, error_bad_lines=False)\n",
    "    resdf.index = [clean_subscripts(n) for n in resdf.index] # Separate subscripts\n",
    "    resdf.replace(-999, np.nan, inplace=True)\n",
    "    datdf = resdf.loc[gof_vars['sim'] + gof_vars['data']]\n",
    "    \n",
    "    # Pull end-of-run values from full country results\n",
    "    endtime = len(resdf.columns) - 1\n",
    "    c_res['cum_deaths'] = resdf.loc['Dead'][-1]\n",
    "    c_res['IFR'] = resdf.loc['IFR'][0]\n",
    "    c_res['SFrac_mdl'] = resdf.loc['Susceptible Fraction'][-1]\n",
    "    \n",
    "    # Calculate response-stringency correlation\n",
    "    try:\n",
    "        response = resdf.loc['Impact of perceived risk on attack rate']\n",
    "        response.index = response.index.astype('int64') # Fix index for concatenation\n",
    "        stringency = table.loc['stringency'].loc[c]\n",
    "        stringency.index = stringency.index.astype('int64') # Fix index for concatenation\n",
    "\n",
    "        # Clean variables for regression to prevent PearsonR failure from NaN\n",
    "        m = pd.concat([response, stringency[:-1]], axis=1).dropna()\n",
    "        response = response[m.index]\n",
    "        stringency = stringency[m.index]\n",
    "        c_res['RS_corr'], c_res['RS_corr_p'] = pearsonr(response, stringency)\n",
    "    except KeyError:\n",
    "        c_res['RS_corr'], c_res['RS_corr_p'] = np.nan, np.nan\n",
    "\n",
    "    # Calculate historical means\n",
    "    for var in means_list:\n",
    "        for lim in hist_windows:\n",
    "            c_res[f\"avg_{var}_{lim}\"] = calc_mean(resdf, var, limit=lim)\n",
    "    \n",
    "    # Calculate GOF statistics\n",
    "    def gof_by_var(df, gof_vars, sfx=''):\n",
    "        maeom = []\n",
    "        r2 = []\n",
    "        for i, (s, d) in enumerate(zip(gof_vars['sim'], gof_vars['data'])):\n",
    "            maeom.append(calc_gof(df, s, d)[0])\n",
    "            r2.append(calc_gof(df, s, d)[2])\n",
    "            c_res[f'maeom{sfx}_{i}'] = np.mean(maeom[i])\n",
    "            c_res[f'r2{sfx}_{i}'] = np.mean(r2[i])\n",
    "        c_res[f'maeom{sfx}'] = np.mean(maeom)\n",
    "        c_res[f'r2{sfx}'] = np.mean(r2)\n",
    "    gof_by_var(resdf, gof_vars)\n",
    "    \n",
    "    for k in fixparams.keys():\n",
    "        fixdf = pd.read_csv(f'{scriptname[:-1]}{k}.tab', sep='\\t', \n",
    "                            index_col=0, error_bad_lines=False)\n",
    "        fixdf.index = [clean_subscripts(n) for n in resdf.index] # Separate subscripts\n",
    "        fixdf.replace(-999, np.nan, inplace=True)\n",
    "        gof_by_var(fixdf, gof_vars, f'_{k}')\n",
    "        for var in means_list:\n",
    "            c_res[f\"{k}_{var}_{hist_windows[-1]}\"] = calc_mean(fixdf, var, limit=hist_windows[-1])\n",
    "    \n",
    "    return c_res, datdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "controlfilename = input(\"Enter control file name (with extension):\")\n",
    "cf = json.load(open(controlfilename, 'r'))\n",
    "\n",
    "# Unpack controlfile into variables\n",
    "for k,v in cf.items():\n",
    "    exec(k + '=v')\n",
    "\n",
    "for setting in [datasettings, analysissettings]:\n",
    "    for k, v in setting.items():\n",
    "        exec(k + '=v')\n",
    "\n",
    "# Set up files in run directory and initialise logfile\n",
    "master = Script(cf)\n",
    "master.changes.extend(scenariolist)\n",
    "master.copy_model_files(f\"{baserunname}_IterCal\")\n",
    "for f in [f\"../{controlfilename}\", \"../ImportData.cmd\", \"../CovRegInput.frm\"]:\n",
    "    copy(f, \"./\")\n",
    "basedir = os.getcwd()\n",
    "basename = cf['baserunname']\n",
    "logfile = f\"{basedir}/{baserunname}.log\"\n",
    "write_log(f\"-----\\nStarting new log at {time.ctime()}\\nReady to work!\", logfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data updating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### RAW DATA INTAKE & PRE-PROCESSING #####\n",
    "\n",
    "# Calculate cutoff dates\n",
    "startdate = datetime.date.fromisoformat('2019-12-31')\n",
    "enddate = startdate + datetime.timedelta(days=end_date+1)\n",
    "em_enddate = startdate + datetime.timedelta(days=end_date//7 * 7 - 1) # Excess mortality data only reported weekly\n",
    "\n",
    "if updatedata != 0:\n",
    "    # Read main, mobility, and excess mortality data from URL for raw data CSVs\n",
    "    dat_raw = pd.read_csv(data_url)\n",
    "    mobdata = pd.read_csv(mobdata_url)\n",
    "    em_data = pd.read_csv(emdata_url)\n",
    "\n",
    "    # Extract dictionary mapping of ISO codes to OWID country names\n",
    "    names = dat_raw.filter(['iso_code','location'], axis=1).drop_duplicates()\n",
    "    names.replace({'iso_code': renames}, inplace=True) # Rename unusual ISO codes as needed\n",
    "    c_dict = dict(zip(names['location'], names['iso_code']))\n",
    "\n",
    "    # Extract vaccination-relevant data for later use\n",
    "    dat_em = dat_raw.filter(['iso_code', 'date', 'total_deaths', 'people_vaccinated_per_hundred', 'people_fully_vaccinated_per_hundred'], axis=1)\n",
    "\n",
    "    # Extract excess mortality data from The Economist\n",
    "    em_data = em_data.filter(['iso3c', 'date', 'cumulative_estimated_daily_excess_deaths'], axis=1)\n",
    "    em_data.columns = ['iso_code', 'date', 'cumulative_excess_mortality']\n",
    "\n",
    "    # Subset vaccination & excess mortality data to specific dates needed\n",
    "    dat_em = dat_em.loc[dat_em['date']==str(enddate)].drop('date', axis=1)\n",
    "    em_data = em_data.loc[em_data['date']==str(em_enddate)].drop('date', axis=1)\n",
    "\n",
    "    # Subset main CSV to relevant data fields\n",
    "    data = dat_raw.filter(['iso_code', 'date', 'total_cases', 'new_cases_smoothed', \n",
    "                        'new_deaths_smoothed', 'reproduction_rate', \n",
    "                        'stringency_index', 'population', 'gdp_per_capita', \n",
    "                        'hospital_beds_per_thousand', 'median_age'], axis=1)\n",
    "\n",
    "    # Rename fields as needed\n",
    "    data.columns = ['iso_code','date', 'total_cases_owid', 'new_cases_owid', 'new_deaths_owid', 'Re_est', \n",
    "                    'stringency', 'population', 'gdp_per_capita', 'hosp_beds', 'median_age']\n",
    "\n",
    "    # Read and merge IHME data files\n",
    "    ihme0, ihme1, ihme2, ihme3 = [pd.read_csv(f\"{ihme_url_partial}_{year}.csv\") for year in [2020, 2021, 2022, 2023]]\n",
    "\n",
    "    # Subset CSV to relevant data fields\n",
    "    ihme = [i.filter(['date', 'location_name', 'inf_cuml_mean', 'inf_mean', 'daily_deaths'], axis=1) \n",
    "            for i in [ihme0, ihme1, ihme2, ihme3]]\n",
    "    del ihme0, ihme1, ihme2, ihme3\n",
    "\n",
    "    ihme = pd.concat(ihme)\n",
    "\n",
    "    # Rename fields as needed\n",
    "    ihme.columns = ['date', 'iso_code', 'total_cases_ihme', 'new_cases_ihme', 'new_deaths_ihme']\n",
    "    ihme.replace({'iso_code': c_dict}, inplace=True) # Convert country names to ISO codes\n",
    "    ihme.replace({'iso_code': ihme_c_dict}, inplace=True)\n",
    "\n",
    "    # Merge OWID and IHME data\n",
    "    data = data.merge(ihme, how='left', on=['iso_code', 'date'])\n",
    "\n",
    "    # Designate specified dataset case & death columns for main analysis\n",
    "    if dataset == 'OWID':\n",
    "        data['total_cases'] = data['total_cases_owid']\n",
    "        data['new_cases'] = data['new_cases_owid']\n",
    "        data['new_deaths'] = data['new_deaths_owid']\n",
    "    elif dataset == 'IHME':\n",
    "        data['total_cases'] = data['total_cases_ihme']\n",
    "        data['new_cases'] = data['new_cases_ihme']\n",
    "        data['new_deaths'] = data['new_deaths_ihme']\n",
    "\n",
    "    table = pd.pivot_table(\n",
    "        data, values=['total_cases', 'new_cases', 'new_deaths', 'Re_est', 'stringency', 'population', \n",
    "                      'gdp_per_capita', 'hosp_beds', 'median_age'], index='date', columns='iso_code')\n",
    "    table = table.T\n",
    "    table.index.names = ['field', 'iso_code']\n",
    "    table.columns = pd.to_datetime(table.columns)\n",
    "\n",
    "    # Save next 6 mo of data for forecast comparison\n",
    "    tab_forecast = table.iloc[:, end_date+1:(end_date + max(hist_windows) + 1)]\n",
    "    table = table.iloc[:, :end_date+1] # Then cut off table at end date\n",
    "\n",
    "    # Drop countries with fewer cases than specified threshold, insufficient datapoints, or zero deaths\n",
    "    dropidx_cases = table.loc['total_cases'].index[table.loc['total_cases'].max(axis=1) < min_cases]\n",
    "    dropidx_deaths = table.loc['new_deaths'].index[table.loc['new_deaths'].max(axis=1) == 0]\n",
    "    first_idxs = (table.loc['total_cases'] > start_cases).idxmax(axis=1)\n",
    "    dropidx_data = table.loc['total_cases'].index[\n",
    "        (table.columns[-1] - first_idxs).dt.days < min_datapoints]\n",
    "    print(\"Insufficient cases: \", dropidx_cases, \"\\nInsufficient deaths: \", dropidx_deaths, \n",
    "          \"\\nInsufficient datapoints: \", dropidx_data, \"\\nMisc. removals: \", droplist)\n",
    "    for drops in [dropidx_cases, dropidx_deaths, dropidx_data, droplist]:\n",
    "        table.drop(drops, level='iso_code', inplace=True, errors='ignore')\n",
    "        tab_forecast.drop(drops, level='iso_code', inplace=True, errors='ignore')\n",
    "\n",
    "    table = table.rename(index=renames) # Rename any unusual ISO codes as needed\n",
    "    tab_forecast = tab_forecast.rename(index=renames)\n",
    "\n",
    "    # Convert column indices to day number since startdate\n",
    "    table.columns = (table.columns - pd.to_datetime('2019-12-31')).days\n",
    "    tab_forecast.columns = (tab_forecast.columns - pd.to_datetime('2019-12-31')).days\n",
    "\n",
    "    # Reorder multiindex levels before by-country subsetting\n",
    "    table = table.reorder_levels(['iso_code', 'field']).sort_index()\n",
    "\n",
    "    # Identify first date over infection threshold for each country and subset dataframe accordingly\n",
    "    dropidx_errors = []\n",
    "    r0dict = {}\n",
    "    for i in table.index.levels[0]:\n",
    "        try:\n",
    "            first_idx = get_first_idx(table.loc[i].loc['total_cases'], start_cases)\n",
    "            table.loc[i].loc[:, :first_idx] = np.NaN\n",
    "            r0dict[i] = table.loc[i].loc['Re_est', max( # Get first point Re value (+1 to avoid NaN)\n",
    "                first_idx+1, table.loc[i].loc['Re_est'].first_valid_index())]\n",
    "        except KeyError:\n",
    "            table.drop(i, level='iso_code', inplace=True, errors='ignore')\n",
    "            dropidx_errors.append(i)\n",
    "    print(\"Other data errors: \", dropidx_errors)\n",
    "    r0 = pd.Series(r0dict) # Create series of first point Re values (initial RR, i.e. R0)\n",
    "\n",
    "    # Clean infinite values and switch multiindex levels back\n",
    "    table.replace([np.inf, -np.inf], np.NaN, inplace=True)\n",
    "    table = table.reorder_levels(['field', 'iso_code']).sort_index()\n",
    "\n",
    "    # If applicable, drop early data for sensitivity analysis\n",
    "    if early_data_cutoff != 0:\n",
    "        table = table.loc[:, early_data_cutoff+1:]\n",
    "\n",
    "    # Move country statistics columns to separate dataframe\n",
    "    statdict = {'R0_est': r0}\n",
    "    for stat in ['population', 'gdp_per_capita', 'hosp_beds', 'median_age']:\n",
    "        statdict[stat] = table.loc[stat].mean(axis=1)\n",
    "        table.drop(stat, level='field', inplace=True, errors='ignore')\n",
    "\n",
    "    cstats = pd.concat(statdict, axis=1)\n",
    "\n",
    "    # Calculate aggregate data over last `hist_window` days and add to country stats dataframe\n",
    "    for lim in hist_windows:\n",
    "        cstats[f'hist_deaths_{lim}'] = table.loc['new_deaths'].iloc[:, -lim:].mean(axis=1)\n",
    "        cstats[f'hist_Re_{lim}'] = table.loc['Re_est'].iloc[:, -lim:].mean(axis=1)\n",
    "        cstats[f'hist_str_{lim}'] = table.loc['stringency'].iloc[:, -lim:].mean(axis=1)\n",
    "        cstats[f'hist_dpm_{lim}'] = cstats[f'hist_deaths_{lim}'] / cstats['population'] * 1e06\n",
    "\n",
    "        # As well as aggregate data from forecast window\n",
    "        cstats[f'next_deaths_{lim}'] = tab_forecast.loc['new_deaths'].iloc[:, :lim].mean(axis=1)\n",
    "        cstats[f'next_Re_{lim}'] = tab_forecast.loc['Re_est'].iloc[:, :lim].mean(axis=1)\n",
    "        cstats[f'next_str_{lim}'] = tab_forecast.loc['stringency'].iloc[:, :lim].mean(axis=1)\n",
    "        cstats[f'next_dpm_{lim}'] = cstats[f'next_deaths_{lim}'] / cstats['population'] * 1e06\n",
    "\n",
    "    # Extract mobility change data to pivot table\n",
    "    mobdata['average'] = pd.concat([mobdata['retail_and_recreation'], mobdata['workplaces']], \n",
    "                                   axis=1).mean(axis=1) # Get average of R&R and workplace values\n",
    "    mobdata.replace({'Country': c_dict}, inplace=True) # Convert country names to ISO codes\n",
    "    mobtable = pd.pivot_table(mobdata, values=['retail_and_recreation', 'workplaces', 'average'], \n",
    "                              index='Year', columns='Country')\n",
    "    mobtable = mobtable.T\n",
    "\n",
    "    # Calculate averages over last `hist_window` days & recompile into new dataframe\n",
    "    mobend = end_date + 1 - mobtable.columns[0] # Mobility data starts 17 Feb 2020\n",
    "    mobstart = mobend - hist_windows[-1]\n",
    "    mobtable = mobtable[mobtable.columns[mobstart:mobend]]\n",
    "    tbm = mobtable.mean(axis=1)\n",
    "    mobmean = pd.concat([tbm.loc['average'], tbm.loc['retail_and_recreation'], tbm.loc['workplaces']], \n",
    "                        keys=['mob_avg', 'mob_rr', 'mob_wk'], axis=1)\n",
    "\n",
    "    cstats = cstats.merge(mobmean, how='left', left_index=True, right_index=True)\n",
    "\n",
    "    # Extract countries over vaccination threshold to save separately\n",
    "    data_vax = pd.concat([dat_em.loc[dat_em['iso_code']=='OWID_WRL'], \n",
    "                          (dat_em.loc[dat_em['iso_code'].isin(table.index.get_level_values(1))]\n",
    "                           .loc[dat_em['people_vaccinated_per_hundred'] >= vax_threshold])])\n",
    "\n",
    "    # Combine excess mortality data and calculate EM threshold flags\n",
    "    dat_em = dat_em.merge(em_data, how='left', on='iso_code')\n",
    "    dat_em['excess_mortality_fraction'] = dat_em['cumulative_excess_mortality'] / dat_em['total_deaths']\n",
    "    dat_em['em_100'] = dat_em['excess_mortality_fraction'] < 2 # Indicates EM < 100% more than reported deaths\n",
    "    dat_em['em_50'] = dat_em['excess_mortality_fraction'] < 1.5\n",
    "    dat_em['em_25'] = dat_em['excess_mortality_fraction'] < 1.25\n",
    "\n",
    "    cstats = cstats.merge(dat_em.set_index('iso_code'), how='left', left_index=True, right_index=True)\n",
    "\n",
    "    # Export processed dataframes to .tab and import to VDF\n",
    "    table.to_csv('./InputData.tab', sep='\\t')\n",
    "    subprocess.run(f\"{vensim7path} \\\"./ImportData.cmd\\\"\", check=True)\n",
    "    data_vax.to_csv('./VaxData.tab', sep='\\t')\n",
    "    cstats.to_csv('./CountryStats.tab', sep='\\t')\n",
    "\n",
    "else: # Or read in existing .tab\n",
    "    table = pd.read_csv(f'./InputData.tab', sep='\\t', index_col=[0,1])\n",
    "    cstats = pd.read_csv('./CountryStats.tab', sep='\\t', index_col=0)\n",
    "\n",
    "display(table)\n",
    "display(cstats)\n",
    "    \n",
    "# Update FinalTime cin with last day of available data - IMPORTANT! USES FIRST FILE IN CHANGES LIST\n",
    "finaltime = len(table.columns)-1\n",
    "with open(simsettings['changes'][0], 'w') as f:\n",
    "    f.write(f\"FINAL TIME = {finaltime}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main model calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### MAIN ANALYSIS, DURATION SENSITIVITY & RESULTS-PROCESSING CODE #####\n",
    "\n",
    "os.chdir(basedir)\n",
    "\n",
    "# Pull country list from data table\n",
    "table.index = table.index.remove_unused_levels()\n",
    "countrylist = list(table.index.levels[1])\n",
    "# countrylist = ['FRA', 'GBR', 'USA', 'ZAF']\n",
    "\n",
    "print(countrylist)\n",
    "\n",
    "# Loop through disease duration values to test, starting with main then sensitivity values\n",
    "for i in ([main_dur] + sens_durs):\n",
    "    cf['baserunname'] = f'{basename}{i}'\n",
    "    baserunname = cf['baserunname']\n",
    "    print(baserunname)\n",
    "\n",
    "    # Create script object for given duration, to cleanly create calibration subfolder\n",
    "    sub = Script(cf)\n",
    "    sub.changes.extend(scenariolist)\n",
    "    sub.copy_model_files(baserunname)\n",
    "    copy(f\"../{controlfilename}\", \"./\")\n",
    "    \n",
    "    # Overwrite disease duration cin file - IMPORTANT! USES LAST FILE IN CHANGES LIST\n",
    "    with open(simsettings['changes'][-1], 'w') as f:\n",
    "        f.write(f\"DiseaseDuration = {i}\")\n",
    "        \n",
    "    dur = i # Assign disease duration variable\n",
    "    \n",
    "    # Initialise necessary .mdl and .voc files\n",
    "    create_mdls(cf, countrylist, finaltime, logfile)\n",
    "    \n",
    "    # Run country-by-country calibration process, unless otherwise specified (mccores=0)\n",
    "    if mccores != 0:\n",
    "        write_log(f\"Initialising MCMC with duration {dur}!\", logfile)\n",
    "        c_list = []\n",
    "        err_list = []\n",
    "\n",
    "        for c in countrylist:\n",
    "            while True:\n",
    "                try:\n",
    "                    res_i = compile_script(cf, CtyScript, c, 'i', {'model': f'_{c}', 'optparm': '_c'}, \n",
    "                                           logfile, subdir=c)\n",
    "                    if res_i != False:\n",
    "                        res_nbr = compile_script(\n",
    "                            cf, CtyScript, c, 'nbr', {'model': f'_{c}', 'optparm': '_nbr'}, \n",
    "                            logfile, subdir=c)\n",
    "                        c_list.append(c) # Compile updated c_list of successful calibrations\n",
    "                    else:\n",
    "                        err_list.append(c) # Compile error list of failed calibrations\n",
    "                    time.sleep(1)\n",
    "                    break\n",
    "                except FileNotFoundError:\n",
    "                    os.chdir(f\"{basedir}/{baserunname}\")\n",
    "                    continue\n",
    "                \n",
    "        write_log(f\"Calibration complete! Error list is:\\n{err_list}\", logfile)\n",
    "    \n",
    "    # If calibration not needed, default to using country list from data as c_list\n",
    "    else:\n",
    "        write_log(\"Hang on to outdated imperialist dogma! Using previous output...\", logfile)\n",
    "        c_list = countrylist\n",
    "        err_list = []\n",
    "    \n",
    "    os.chdir(\"..\") # Remember to go back to root directory before next iteration!\n",
    "\n",
    "##### PROCESS CALIBRATION RESULTS #####\n",
    "write_log(\"Processing results!\", logfile)\n",
    "\n",
    "# Initialise containers for processed country results and death data\n",
    "res_list = []\n",
    "dat_list = []\n",
    "\n",
    "# Loop through country MCMC outputs, calling master results processing function on each\n",
    "for c in c_list:\n",
    "    try:\n",
    "        print(f\"Processing {c}!\")\n",
    "        c_res, datdf = process_results(f'./{c}/{baserunname}_{c}_i', gof_vars, c, iqr_list, \n",
    "                                       means_list, hist_windows, perc_list, delta, dur)\n",
    "        print(f\"{c} successful!\")\n",
    "        res_list.append(c_res)\n",
    "        dat_list.append(datdf)\n",
    "    except FileNotFoundError:\n",
    "        err_list.append(c)\n",
    "\n",
    "display(err_list)\n",
    "\n",
    "# Compile main results dataframe with processed country results\n",
    "results = pd.concat(res_list, axis=1)\n",
    "\n",
    "# Compile country infection and death outputs over time\n",
    "gof_results = [\n",
    "    pd.concat([df.loc[var] for df in dat_list], axis=1) for var \n",
    "    in gof_vars['sim'] + gof_vars['data']]\n",
    "\n",
    "# Assign results dataframe indices based on c_list\n",
    "for df in [results] + gof_results:\n",
    "    df.columns = [c for c in c_list if c not in err_list]\n",
    "results = results.T\n",
    "gof_results = [df.T for df in gof_results]\n",
    "\n",
    "# Recompile results dataframe with aggregate data previously separated\n",
    "results = results.merge(cstats, how='left', left_index=True, right_index=True)\n",
    "\n",
    "# Calculate additional results\n",
    "results['alp[Death]'] = results['alp[Infection]'] * results['alpratio']\n",
    "#     results['Alpha K'] = results['Alpha 0'] * (results['Alpha Rel'] - 1) / end_date\n",
    "for lim in hist_windows:\n",
    "    results[f\"avg_dpm_{lim}\"] = results[f\"avg_Outputs[Death]_{lim}\"] / results['population'] * 1e06\n",
    "\n",
    "results['hist_dpm_norm'] = results['hist_dpm_180'] / results['IFR'] * results['IFR'].mean()\n",
    "results['avg_dpm_norm'] = results['avg_dpm_180'] / results['IFR'] * results['IFR'].mean()\n",
    "results['next_dpm_norm'] = results['next_dpm_180'] / results['IFR'] * results['IFR'].mean()\n",
    "results['g_D_180'] = 1 / (1 + results['avg_Sensitivity Alpha_180'] * results['avg_dpm_180'])\n",
    "results['marginal'] = 1 / (1 + results['avg_Sensitivity Alpha_180'])\n",
    "results['half_contact_dpm'] = 1/ results['avg_Sensitivity Alpha_180']\n",
    "\n",
    "display(results)\n",
    "\n",
    "gofdf = pd.concat(gof_results, keys=gof_vars['sim']+gof_vars['data'])\n",
    "gofdf.index.names=['field', 'iso_code']\n",
    "display(gofdf)\n",
    "\n",
    "\n",
    "results.to_csv(f'./{baserunname}_results.tab', sep='\\t')\n",
    "gofdf.to_csv(f'./{baserunname}_fits.tab', sep='\\t')\n",
    "copy(f'./{baserunname}_results.tab', '../')\n",
    "copy(f'./{baserunname}_fits.tab', '../')\n",
    "\n",
    "os.chdir(\"..\") # Remember to go back to root directory before next iteration!\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphing & further results processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(basedir)\n",
    "figext = 'png'\n",
    "r = pd.read_csv(f'./{baserunname}_results.tab', sep='\\t', index_col=0)\n",
    "# Exclude outliers where responsiveness failed to estimate accurately\n",
    "r = r[r['avg_Sensitivity Alpha_180'] > 0.002]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Paper Exhibit 1 & Figures S5, S6 (forecast deaths vs. responsiveness scatter) ###\n",
    "\n",
    "xvar = 'avg_Sensitivity Alpha_180'\n",
    "yvar = 'next_dpm_180'\n",
    "threshold = 5e07\n",
    "\n",
    "fig0, ax0 = plt.subplots(figsize=[7.5, 7], constrained_layout=True)\n",
    "\n",
    "# Format X and Y axes with log scales if specified\n",
    "ax0.set_xscale('log')\n",
    "ax0.xaxis.set_major_formatter(mticker.FuncFormatter(lambda x, _: '{:g}'.format(x)))\n",
    "ax0.set_yscale('log')\n",
    "ax0.yaxis.set_major_formatter(mticker.FuncFormatter(lambda y, _: '{:g}'.format(y)))\n",
    "\n",
    "# Plot basic scatterplot of all countries\n",
    "ax0.scatter(r[xvar], r[yvar], c='gray')\n",
    "\n",
    "# Plot highlight scatterplot of countries with population above `threshold`\n",
    "p = r[r['population'] > threshold]\n",
    "p = p[p['em_100'] == True]\n",
    "ax0.scatter(p[xvar], p[yvar])\n",
    "for i in p.index: # Label points with country abbreviations\n",
    "    ax0.annotate(f' {i}', (p[xvar][i], p[yvar][i]), fontsize=8)\n",
    "\n",
    "# Take log of variables for regression if specified\n",
    "x = trunc_log(r[xvar])\n",
    "y = trunc_log(r[yvar])\n",
    "\n",
    "# Clean variables for regression to prevent PearsonR failure from NaN\n",
    "m = pd.concat([x, y], axis=1).dropna()\n",
    "x = x[m.index]\n",
    "y = y[m.index]\n",
    "\n",
    "# Fit basic OLS model\n",
    "mod_OLS = sm.OLS(y, sm.add_constant(x), missing='drop')\n",
    "fit_reg = mod_OLS.fit()\n",
    "display(fit_reg.params)\n",
    "display(fit_reg.pvalues)\n",
    "\n",
    "# Plot best-fit regression line, log-scaled if necessary\n",
    "# ax0.plot(r[xvar][m.index], np.exp(fit_reg.fittedvalues), c='r', ls='dotted')\n",
    "\n",
    "# Label X and Y axis\n",
    "ax0.set_xlabel(r'Responsiveness $\\alpha$', fontsize=12)\n",
    "ax0.set_ylabel('Deaths per million (next 6 mo avg)', fontsize=12)\n",
    "\n",
    "# Set up and annotate with summary stats\n",
    "# stats = (f\"Slope = {'{0:.3g}'.format(fit_reg.params[1])}\\n\"\n",
    "#          r\"$R^2$\" f\" = {'{0:.3g}'.format(fit_reg.rsquared)}\")\n",
    "os_corr = '{0:.3g}'.format(x.corr(y))\n",
    "stats = (f\"Correlation = {os_corr}\")\n",
    "ax0.text(0.05, 0.05, stats, transform=ax0.transAxes, backgroundcolor='white', fontsize=14, horizontalalignment='left')\n",
    "\n",
    "fig0.savefig(f\"./{baserunname}_dpm_alpha_next_noline.{figext}\", bbox_inches='tight')\n",
    "# fig0.savefig(f\"./{baserunname}_dpm_alpha_next.{figext}\", bbox_inches='tight')\n",
    "\n",
    "alpha_r2 = (xvar, yvar, fit_reg.rsquared)\n",
    "\n",
    "# Save figure data to Excel for plot\n",
    "r[[xvar, yvar]].to_excel(f'./{baserunname}_excelfig.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Paper Figure S3 (additional potential explanatory variables) ###\n",
    "\n",
    "def plot_partial(r, ax, xvar, yvar, xlog=False, ylog=False, xticks=False, x_label='', y_label='', threshold=0):\n",
    "    # Format X and Y axes with log scales if specified\n",
    "    if xlog == True:\n",
    "        ax.set_xscale('log')\n",
    "        ax.xaxis.set_major_formatter(mticker.FuncFormatter(lambda x, _: '{:g}'.format(x)))\n",
    "    if xticks == True:\n",
    "        ax.xaxis.set_major_formatter(mticker.FuncFormatter(lambda x, _: '{:g}%'.format(x * 100)))\n",
    "        ax.xaxis.set_minor_formatter(mticker.FuncFormatter(lambda x, _: '{:g}%'.format(x * 100)))\n",
    "        ax.tick_params(axis='x', which='both', labelsize=8)\n",
    "    if ylog == True:\n",
    "        ax.set_yscale('log')\n",
    "        ax.yaxis.set_major_formatter(mticker.FuncFormatter(lambda y, _: '{:g}'.format(y)))\n",
    "\n",
    "    # Plot basic scatterplot of all countries\n",
    "    ax.scatter(r[xvar], r[yvar], c='gray', alpha=0.5)\n",
    "\n",
    "    # Plot highlight scatterplot of countries with population above `threshold`\n",
    "    p = r[r['population'] > threshold]\n",
    "    ax.scatter(p[xvar], p[yvar])\n",
    "    for i in p.index: # Label points with country abbreviations\n",
    "        ax.annotate(f' {i}', (p[xvar][i], p[yvar][i]), fontsize=8)\n",
    "\n",
    "    # Take log of variables for regression if specified\n",
    "    if xlog == True:\n",
    "        x = trunc_log(r[xvar])\n",
    "    else: x = r[xvar]\n",
    "    if ylog == True:\n",
    "        y = trunc_log(r[yvar])\n",
    "    else: y = r[yvar]\n",
    "\n",
    "    # Clean variables for regression to prevent PearsonR failure from NaN\n",
    "    m = pd.concat([x, y], axis=1).dropna()\n",
    "    x = x[m.index]\n",
    "    y = y[m.index]\n",
    "\n",
    "    # Fit basic OLS model\n",
    "    mod_OLS = sm.OLS(y, sm.add_constant(x), missing='drop')\n",
    "    fit_reg = mod_OLS.fit()\n",
    "    display(fit_reg.params)\n",
    "\n",
    "    # Plot best-fit regression line, log-scaled if necessary\n",
    "    if ylog == True:\n",
    "        ax.plot(r[xvar][m.index], np.exp(fit_reg.fittedvalues), c='r', ls='dotted')\n",
    "    else:\n",
    "        ax.plot(r[xvar][m.index], fit_reg.fittedvalues, c='r', ls='dotted')\n",
    "        \n",
    "    # Label X and Y axis\n",
    "    ax.set_xlabel(x_label, fontsize=12)\n",
    "    ax.set_ylabel(y_label, fontsize=12)\n",
    "\n",
    "    # Set up and annotate with summary stats\n",
    "    stats = (f\"Slope = {'{0:.3g}'.format(fit_reg.params[1])}\\n\"\n",
    "             r\"$R^2$\" f\" = {'{0:.3g}'.format(fit_reg.rsquared)}\")\n",
    "    ax.text(0.95, 0.05, stats, transform=ax.transAxes, backgroundcolor='white', fontsize=12, ha='right')\n",
    "    \n",
    "    return (xvar, yvar, fit_reg.rsquared)\n",
    "\n",
    "\n",
    "# Set up basic figure and axes\n",
    "fig0, ((ax0, ax1, ax2), (ax3, ax4, ax5)) = plt.subplots(2, 3, figsize=[18, 12], constrained_layout=True, sharey=True)\n",
    "\n",
    "alp_r2 = plot_partial(r, ax0, 'avg_Sensitivity Alpha_180', 'next_dpm_180', True, True, False, \n",
    "                      r'Responsiveness $\\alpha$', 'Deaths per million (prediction period avg)', threshold=5e07)\n",
    "IFR_r2 = plot_partial(r, ax1, 'IFR', 'next_dpm_180', True, True, True,  \n",
    "                      'Infection fatality rate (%)', threshold=5e07)\n",
    "GDP_r2 = plot_partial(r, ax2, 'gdp_per_capita', 'next_dpm_180', True, True, False,  \n",
    "                      'GDP per capita', threshold=5e07)\n",
    "hosp_r2 = plot_partial(r, ax3, 'hosp_beds', 'next_dpm_180', False, True, False, \n",
    "                       'Hospital beds per thousand people', 'Deaths per million (prediction period avg)', threshold=5e07)\n",
    "R0_r2 = plot_partial(r, ax4, 'R0_est', 'next_dpm_180', False, True, False, \n",
    "                     r'Initial reproduction number $R_0$', threshold=5e07)\n",
    "str_r2 = plot_partial(r, ax5, 'next_str_180', 'next_dpm_180', False, True, False, \n",
    "                      'OxCGRT stringency index (6 month average)', threshold=5e07)\n",
    "\n",
    "for ax, letter in zip([ax0, ax1, ax2, ax3, ax4, ax5], ['A', 'B', 'C', 'D', 'E', 'F']):\n",
    "    ax.text(0.03, 0.97, letter, transform=ax.transAxes, backgroundcolor='white', \n",
    "            fontsize=12, weight='bold', va='top')\n",
    "\n",
    "fig0.savefig(f\"./{baserunname}_dpm_corrs_exp.{figext}\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Paper Figure S4 (mobility scatterplot) ###\n",
    "fig0, (ax0, ax1) = plt.subplots(1, 2, figsize=[12, 6], constrained_layout=True, sharey=True)\n",
    "\n",
    "mobrr_r2 = plot_partial(r, ax0, 'mob_rr', 'avg_dpm_180', False, True, False, \n",
    "                        'Change in mobility, retail & recreation (6 mo avg)', 'Deaths per million (6 mo avg, estimated)', threshold=5e07)\n",
    "mobwk_r2 = plot_partial(r, ax1, 'mob_wk', 'avg_dpm_180', False, True, False, \n",
    "                        'Change in mobility, workplaces (6 mo avg)', threshold=5e07)\n",
    "\n",
    "for ax, letter in zip([ax0, ax1], ['A', 'B']):\n",
    "    ax.text(0.03, 0.97, letter, transform=ax.transAxes, backgroundcolor='white', \n",
    "            fontsize=12, weight='bold', va='top')\n",
    "\n",
    "fig0.savefig(f\"./{baserunname}_mobility_supp.{figext}\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Paper Exhibit 2 (main mortality regression) & Tables S6-S12 (robustness checks) ###\n",
    "\n",
    "params = ['Beta', 'Alpha 0', 'Time to increase risk', 'Time to reduce risk', \n",
    "          'Patient Zero Arrival Time', 'alp[Infection]', 'alp[Death]']\n",
    "\n",
    "paramdict = {}\n",
    "for p in params:\n",
    "    paramdict[p] = {'Mean': '{0:.3g}'.format(r[p].mean()), \n",
    "                    'Median': '{0:.3g}'.format(r[p].median()), \n",
    "                    'StDev': '{0:.3g}'.format(r[p].std())}\n",
    "paramdf = pd.DataFrame(paramdict).T\n",
    "paramdf.to_csv(f'./{baserunname}_params.tab', sep='\\t')\n",
    "\n",
    "pd.set_option('mode.chained_assignment', None) # Suppress warnings\n",
    "\n",
    "for frac in ['all', '100', '50', '25', 'vax_part', 'vax_full']:\n",
    "    # Define predictor variables to include in regression, log-transformed or not\n",
    "    reg_list = ['hosp_beds', 'R0_est', 'next_str_180']\n",
    "    reg_log_list = ['avg_Sensitivity Alpha_180', 'IFR', 'gdp_per_capita']\n",
    "\n",
    "    # Drop countries with missing data & subset by excess mortality fraction\n",
    "    if frac == 'all':\n",
    "        t = r.dropna(subset=reg_list + reg_log_list + ['next_dpm_180'])\n",
    "    elif frac == 'vax_full':\n",
    "        t = (r[(r['people_fully_vaccinated_per_hundred'] < vax_threshold)\n",
    "               |(pd.isnull(r['people_fully_vaccinated_per_hundred']))]\n",
    "             .loc[r[f'em_100']==True]\n",
    "             .dropna(subset=reg_list + reg_log_list + ['next_dpm_180']))\n",
    "    elif frac == 'vax_part':\n",
    "        t = (r[(r['people_vaccinated_per_hundred'] < vax_threshold)\n",
    "               |(pd.isnull(r['people_vaccinated_per_hundred']))]\n",
    "             .loc[r[f'em_100']==True]\n",
    "             .dropna(subset=reg_list + reg_log_list + ['next_dpm_180']))\n",
    "    else:\n",
    "        t = r.loc[r[f'em_{frac}']==True].dropna(subset=reg_list + reg_log_list + ['next_dpm_180'])\n",
    "\n",
    "    # Take log for outcome and specified predictors\n",
    "    for var in reg_log_list + ['next_dpm_180']:\n",
    "        t.loc[:, f'log_{var}'] = trunc_log(t[var])\n",
    "\n",
    "    # Compile list of predictor variables\n",
    "    varlist = [f'log_{var}' for var in reg_log_list] + reg_list\n",
    "\n",
    "    # Drop countries with errors from log transform\n",
    "    t.dropna(subset=varlist + ['log_next_dpm_180'], inplace=True)\n",
    "\n",
    "    # Specify predictors & outcome for more compact code\n",
    "    y = t['log_next_dpm_180']\n",
    "    x = t[varlist]\n",
    "\n",
    "    # Define whether to add constant or not\n",
    "    reg_const = 0\n",
    "\n",
    "    # Create and fit OLS, including intercept if specified\n",
    "    if reg_const:\n",
    "        mod_OLS_f = sm.OLS(y, sm.add_constant(x), missing='drop')\n",
    "    else:\n",
    "        mod_OLS_f = sm.OLS(y, x, missing='drop')\n",
    "    fit_reg_f = mod_OLS_f.fit()\n",
    "    display(fit_reg_f.summary())\n",
    "\n",
    "    # Save full results\n",
    "    with open(f'{baserunname}_reg_raw_{frac}.csv', 'w') as f:\n",
    "        f.write(fit_reg_f.summary().as_csv())\n",
    "\n",
    "    # Record R2 and adjusted R2 values\n",
    "    r2_f, r2a_f = fit_reg_f.rsquared, fit_reg_f.rsquared_adj\n",
    "\n",
    "    # Initialise containers for results\n",
    "    r2dict, r2adict, effdict, efldict, efhdict = [pd.Series() for i in range(5)]\n",
    "\n",
    "    # Drop each individual predictors and re-fit model to calculate contributions\n",
    "    for var in varlist:\n",
    "        sublist = [v for v in varlist if v != var]\n",
    "        x = t[sublist]\n",
    "\n",
    "        if reg_const:\n",
    "            mod_OLS = sm.OLS(y, sm.add_constant(x), missing='drop')\n",
    "        else:\n",
    "            mod_OLS = sm.OLS(y, x, missing='drop')\n",
    "        fit_reg = mod_OLS.fit()\n",
    "\n",
    "        r2dict[var] = r2_f - fit_reg.rsquared\n",
    "        r2adict[var] = r2a_f - fit_reg.rsquared_adj\n",
    "\n",
    "        # Calculate effect sizes and bounds\n",
    "        effdict[var] = 10 ** (fit_reg_f.params[var] * (t[var].std()))\n",
    "        efldict[var] = 10 ** ((fit_reg_f.params[var] - 1.96 * fit_reg_f.HC0_se[var]) * (t[var].std()))\n",
    "        efhdict[var] = 10 ** ((fit_reg_f.params[var] + 1.96 * fit_reg_f.HC0_se[var]) * (t[var].std()))\n",
    "\n",
    "    # Define and concatenate relevant results variables\n",
    "    r_c, r_se, r_t, r_p = fit_reg_f.params, fit_reg_f.HC0_se, fit_reg_f.tvalues, fit_reg_f.pvalues\n",
    "    reg_res = pd.concat([r_c, r_se, r_t, r_p, r2dict, r2adict, effdict, efldict, efhdict], keys=[\n",
    "        'Coeff', 'Std Err', 't', 'P>|t|', r'$R^2$', r'Adj. $R^2$', 'Eff. size', 'Eff. 2.5%', 'Eff 97.5%'], axis=1)\n",
    "\n",
    "    # Save regression results & predictor correlation matrix\n",
    "    reg_res.to_csv(f'./{baserunname}_regression_{frac}.tab', sep='\\t')\n",
    "    t[varlist].corr().to_csv(f'./{baserunname}_regcorrs_{frac}.tab', sep='\\t')\n",
    "    display(reg_res)\n",
    "    display(t[varlist].corr())\n",
    "\n",
    "    # Calculate and save variance inflation factors\n",
    "    xdf = sm.add_constant(t[varlist])\n",
    "    vifs = pd.Series([variance_inflation_factor(xdf.values, i) \n",
    "                      for i in range(xdf.shape[1])], index=xdf.columns)\n",
    "    vifs.drop('const', inplace=True)\n",
    "    vifs.to_csv(f'./{baserunname}_vifs_{frac}.tab', sep='\\t')\n",
    "    display(vifs)\n",
    "\n",
    "pd.set_option('mode.chained_assignment', 'warn') # Unsuppress warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### CREATE KEY RESULTS SUMMARY FILE #####\n",
    "with open(cf['simsettings']['optparm']) as f:\n",
    "    restartline = [line for line in f.readlines() if ':RESTART_MAX' in line]\n",
    "    restarts = int(restartline[0].split('=')[1])\n",
    "\n",
    "# Compile summarytext\n",
    "summarytext = [\n",
    "    f\"Total countries\\t{len(r.index)}\\tPopulation\\t{int(r['population'].sum())}\\n\", \n",
    "    f\"Dates\\t{startdate.isoformat()}\\t{enddate.isoformat()}\\tDays\\t{end_date}\\n\", \n",
    "    f\"Min cumulative cases\\t{min_cases}\\tMin datapoints\\t{min_datapoints}\\tStartpoint cases\\t{start_cases}\\n\", \n",
    "    f\"Restarts\\t{restarts}\\n\", \n",
    "    f\"Hist DPM\\t{r['hist_dpm_180'].quantile(0.05)}\\t{r['hist_dpm_180'].median()}\\t{r['hist_dpm_180'].quantile(0.95)}\\n\", \n",
    "    f\"IFR\\t{r['IFR'].quantile(0.05)}\\t{r['IFR'].median()}\\t{r['IFR'].quantile(0.95)}\\n\", \n",
    "    f\"R0_est\\t{r['R0_est'].quantile(0.05)}\\t{r['R0_est'].median()}\\t{r['R0_est'].quantile(0.95)}\\n\", \n",
    "    f\"Alpha\\t{r['avg_Sensitivity Alpha_180'].quantile(0.05)}\\t{r['avg_Sensitivity Alpha_180'].median()}\\t{r['avg_Sensitivity Alpha_180'].quantile(0.95)}\\n\", \n",
    "    f\"In-sample stats\\tCorrelation\\t{is_corr}\\tSlope\\t{is_slope}\\tR2\\t{is_r2}\\n\", \n",
    "    f\"Out-of-sample stats\\tCorrelation\\t{os_corr}\\n\", \n",
    "    f\"Response-stringency correlation\\tMean\\t{r['RS_corr'].mean()}\\tMedian\\t{r['RS_corr'].median()}\\n\", \n",
    "    f\"Excess mortality thresholds\\tEM100\\t{sum(r['em_100'])}\\tEM50\\t{sum(r['em_50'])}\\tEM25\\t{sum(r['em_25'])}\\n\", \n",
    "    f\"Countries w/ >10% vax\\tPartial vax\\t{len(r.loc[r['people_vaccinated_per_hundred'] >= 10])}\\tFull vax\\t{len(r.loc[r['people_fully_vaccinated_per_hundred'] >= 10])}\\n\"\n",
    "]\n",
    "    \n",
    "# Export summary text file\n",
    "with open(f\"{baserunname}_summary.tab\", 'w') as summaryfile:\n",
    "    summaryfile.writelines(summarytext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### End of main analysis\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "000000000000000000000000000000000000000000000000000000000000000000000000\n",
    "000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
